{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a302084-ebc6-46b2-b128-2ec938b279aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def reload_vars(filename):\n",
    "    this_var = None\n",
    "    with open(filename, 'rb') as file:\n",
    "        this_var = pickle.load(file)\n",
    "        \n",
    "    return this_var\n",
    "\n",
    "def pkl_vars(varname, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(varname, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "449679d2-ed31-4a07-b79c-72c394249c67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 20:44:10.607900: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-03 20:44:10.766462: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-03 20:44:11.297730: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-05-03 20:44:11.297819: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-05-03 20:44:11.297826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "from seqeval import scheme\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from seqeval.metrics import classification_report\n",
    "import math\n",
    "import os\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3baeaf68-f28d-4cc3-8a54-8faf02f09a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 15#3#20\n",
    "LEARNING_RATE = 5e-5 #1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "MAX_LEN = 256\n",
    "model_name = \"dbmdz/bert-base-german-europeana-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e850216-794a-425f-b746-a1c002b3a2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers, logging\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "LOG_FORMAT = '%(asctime)s : %(filename)s : %(funcName)s : %(levelname)s : %(message)s'\n",
    "logging.basicConfig(filename='FT_NER_xlm-roberta-base.log', level=logging.INFO, format=LOG_FORMAT)\n",
    "logger = logging.getLogger(\"FT_NER_xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf62ebbb-05fb-4cc4-83cb-19b82d9d79c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  3 22:42:00 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.39.01    Driver Version: 510.39.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100S-PCI...  On   | 00000000:21:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    35W / 250W |  20032MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100S-PCI...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    25W / 250W |      4MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1731801      C                                    9571MiB |\n",
      "|    0   N/A  N/A   2588677      C                                   10457MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c5b81-24b0-4f3f-a368-a929257a9324",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5f94f80-2396-4836-ac2a-b6f7ba388c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940897d5-eacf-4850-a706-e4be2032c140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9b52f77-d251-430c-b773-0afa73eff84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open(\"data/REF.json\")\n",
    "REF_JSON = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8339b837-1c61-4348-a5a4-100c7bcff34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAW_DATA = []\n",
    "ALL_TAG_COUNTER = Counter()\n",
    "\n",
    "\n",
    "with open('data/ddb-ner-dataset.conll', encoding=\"utf-8\") as f:\n",
    "    sent_id = 0\n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "    for line in f:\n",
    "        if line == \"\" or line == \"\\n\":\n",
    "            if tokens:\n",
    "                RAW_DATA.append({\n",
    "                    \"sentence\": REF_JSON[sent_id][\"title\"],\n",
    "                    \"df_idx\": REF_JSON[sent_id][\"df_idx\"],\n",
    "                    \"tokens\": tokens,\n",
    "                    \"ner_tags\": ner_tags,\n",
    "                })\n",
    "                sent_id += 1\n",
    "                tokens = []\n",
    "                ner_tags = []\n",
    "        else:\n",
    "            # SROIE2019 tokens are space separated\n",
    "            splits = line.split(\" \")\n",
    "            tokens.append(splits[0])\n",
    "            ner_tags.append(splits[3].strip())\n",
    "            ALL_TAG_COUNTER.update({splits[3].strip(): 1})\n",
    "\n",
    "    # last example\n",
    "    RAW_DATA.append({\n",
    "        \"sentence\": REF_JSON[sent_id][\"title\"],\n",
    "        \"df_idx\": REF_JSON[sent_id][\"df_idx\"],\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": ner_tags,\n",
    "\n",
    "    })\n",
    "\n",
    "pkl_vars(RAW_DATA, 'data/vars/RAW_DATA_2025.05.01.pkl')\n",
    "df = pd.DataFrame(RAW_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "284611f9-05d3-49f4-b675-3bceba90db73",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'B-DATE-CREATION',\n",
       " 'B-DATE-PUB',\n",
       " 'B-DATE-SUBJ',\n",
       " 'B-GPE-AUT',\n",
       " 'B-GPE-CREATION',\n",
       " 'B-GPE-DES',\n",
       " 'B-GPE-PUB',\n",
       " 'B-GPE-SUBJ',\n",
       " 'B-LITWORK',\n",
       " 'B-ORG-CREATION',\n",
       " 'B-ORG-DES',\n",
       " 'B-ORG-PUB',\n",
       " 'B-ORG-SUBJ',\n",
       " 'B-PERSON-AUT',\n",
       " 'B-PERSON-CON',\n",
       " 'B-PERSON-EDT',\n",
       " 'B-PERSON-PUB',\n",
       " 'B-PERSON-SUBJ',\n",
       " 'B-YEAR-CREATION',\n",
       " 'B-YEAR-PUB',\n",
       " 'B-YEAR-SUBJ',\n",
       " 'I-DATE-CREATION',\n",
       " 'I-DATE-PUB',\n",
       " 'I-DATE-SUBJ',\n",
       " 'I-GPE',\n",
       " 'I-GPE-CREATION',\n",
       " 'I-GPE-DES',\n",
       " 'I-GPE-PUB',\n",
       " 'I-GPE-SUBJ',\n",
       " 'I-I-GPE-DES',\n",
       " 'I-LITWORK',\n",
       " 'I-ORG',\n",
       " 'I-ORG-CREATION',\n",
       " 'I-ORG-DES',\n",
       " 'I-ORG-PUB',\n",
       " 'I-ORG-SUBJ',\n",
       " 'I-PERSON-AUT',\n",
       " 'I-PERSON-CON',\n",
       " 'I-PERSON-EDT',\n",
       " 'I-PERSON-PUB',\n",
       " 'I-PERSON-SUBJ',\n",
       " 'I-YEAR-CREATION',\n",
       " 'I_ORG-DES',\n",
       " 'O']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_labels = sorted(ALL_TAG_COUNTER.keys())\n",
    "ne_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87928495-1f87-417f-8855-b8b07a21125f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preparing the dataset and dataloader\n",
    "\n",
    "Now that our data is preprocessed, we can turn it into PyTorch tensors such that we can provide it to the model. Let's start by defining some key variables that will be used later on in the training/evaluation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd6af77-7cf4-402b-a0de-2e792036f649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(tokens, ner_tags, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(tokens, ner_tags):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "label2id = reload_vars('data/vars/label2id')\n",
    "id2label = reload_vars('data/vars/id2label')\n",
    "\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        words = self.data.tokens[index]  \n",
    "        word_labels = self.data.ner_tags[index]  \n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(words, word_labels, self.tokenizer)\n",
    "        \n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS] \"] + tokenized_sentence + [\" [SEP]\"] # add special tokens of Roberta\n",
    "        labels.insert(0, 'O') # add outside label for [CLS] token\n",
    "        labels.append('O') # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "            # truncate\n",
    "            tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "            labels = labels[:maxlen]\n",
    "        else:\n",
    "            # pad\n",
    "            tokenized_sentence = tokenized_sentence + ['[pad]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "            labels = labels + ['O' for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[pad]' else 0 for tok in tokenized_sentence] #modifié selon https://huggingface.co/docs/transformers/v4.21.1/en/model_doc/camembert\n",
    "        \n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "        \n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9330b1-5b22-483c-a9c0-40ecf59ac08e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## tests and data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc8f2e98-9370-49b2-983f-d3b1eb44167e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAW_DATA = reload_vars('data/vars/RAW_DATA_2025.05.01.pkl')\n",
    "df = pd.DataFrame(RAW_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75fddc2e-bfc0-4efd-810e-3e5930921fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee35aa1-ac26-4208-b084-33ffef6545df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hochzeit-Schertz  O\n",
      "/                O\n",
      "In               O\n",
      "Vergleichung     O\n",
      "des              O\n",
      "süssen           O\n",
      "Ehstandes        O\n",
      "mit              O\n",
      "dem              O\n",
      "Baw              O\n",
      "des              O\n",
      "lieben           O\n",
      "Berg-Wercks      O\n",
      ":                O\n",
      "Bey              O\n",
      "Des              O\n",
      "...              O\n",
      "Herrn            O\n",
      "Jobst            B-PERSON-SUBJ\n",
      "Sellenstedts     I-PERSON-SUBJ\n",
      "/                O\n",
      "Fürstl.          O\n",
      "Braunschw.       B-GPE-DES\n",
      "und              I-GPE-DES\n",
      "Lünäburg         I-GPE-DES\n",
      ".                O\n",
      "wolbestelten     O\n",
      "Zehnd            O\n",
      ":                O\n",
      "Gegenschreibers  O\n",
      "auff             O\n",
      "der              O\n",
      "Fürstl.          B-GPE-SUBJ\n",
      "Bergstadt        I-GPE-SUBJ\n",
      "Claußthal        I-GPE-SUBJ\n",
      "/                O\n",
      "angetretenen     O\n",
      "andern           O\n",
      "Eh               O\n",
      "/                O\n",
      "Mit              O\n",
      "der              O\n",
      "...              O\n",
      "Jungfer          O\n",
      "Christinen       B-PERSON-SUBJ\n",
      "/                O\n",
      "Des              O\n",
      "...              O\n",
      "Herrn            O\n",
      "Matthias         B-PERSON-SUBJ\n",
      "Tollens          I-PERSON-SUBJ\n",
      "/                O\n",
      "Fürstl.          O\n",
      "Braunschw.       B-GPE-DES\n",
      "Lünäburg         I-GPE-DES\n",
      ".                O\n",
      "Richter          O\n",
      "und              O\n",
      "Hütten-Reuter    O\n",
      "daselbst         O\n",
      "/                O\n",
      "hertzgeliebten   O\n",
      "Tochter          O\n",
      ";                O\n",
      "Bey              O\n",
      "feyerlicher      O\n",
      "Vollziehung      O\n",
      "den              O\n",
      "2.               B-DATE-SUBJ\n",
      "May-Monats       I-DATE-SUBJ\n",
      "im               O\n",
      "1652sten         B-YEAR-SUBJ\n",
      "Jahre            O\n"
     ]
    }
   ],
   "source": [
    "for token, tag in zip(RAW_DATA[0][\"tokens\"], RAW_DATA[0][\"ner_tags\"]):\n",
    "    print('{0:15}  {1}'.format(token, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f96ddf4-d21e-446b-b4c5-25f23eb61481",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Hochzeit        O\n",
      "-                O\n",
      "Sch              O\n",
      "ert              O\n",
      "z                O\n",
      "▁/               O\n",
      "▁In              O\n",
      "▁Vergleich       O\n",
      "ung              O\n",
      "▁des             O\n",
      "▁süs             O\n",
      "sen              O\n",
      "▁Eh              O\n",
      "stand            O\n",
      "es               O\n",
      "▁mit             O\n",
      "▁dem             O\n",
      "▁Ba              O\n",
      "w                O\n",
      "▁des             O\n",
      "▁lieben          O\n",
      "▁Berg            O\n",
      "-                O\n",
      "Wer              O\n",
      "cks              O\n",
      "▁:               O\n",
      "▁Bey             O\n",
      "▁Des             O\n",
      "▁...             O\n",
      "▁Herr            O\n",
      "n                O\n",
      "▁Jobs            B-PERSON-SUBJ\n",
      "t                B-PERSON-SUBJ\n",
      "▁Selle           I-PERSON-SUBJ\n",
      "n                I-PERSON-SUBJ\n",
      "stedt            I-PERSON-SUBJ\n",
      "s                I-PERSON-SUBJ\n",
      "▁/               O\n",
      "▁Für             O\n",
      "st               O\n",
      "l                O\n",
      ".                O\n",
      "▁Braun           B-GPE-DES\n",
      "sch              B-GPE-DES\n",
      "w                B-GPE-DES\n",
      ".                B-GPE-DES\n",
      "▁und             I-GPE-DES\n",
      "▁Lü              I-GPE-DES\n",
      "nä               I-GPE-DES\n",
      "burg             I-GPE-DES\n",
      "▁                O\n",
      ".                O\n",
      "▁wol             O\n",
      "beste            O\n",
      "lt               O\n",
      "en               O\n",
      "▁Ze              O\n",
      "h                O\n",
      "nd               O\n",
      "▁:               O\n",
      "▁Gegen           O\n",
      "schreib          O\n",
      "ers              O\n",
      "▁a               O\n",
      "uff              O\n",
      "▁der             O\n",
      "▁Für             B-GPE-SUBJ\n",
      "st               B-GPE-SUBJ\n",
      "l                B-GPE-SUBJ\n",
      ".                B-GPE-SUBJ\n",
      "▁Berg            I-GPE-SUBJ\n",
      "stadt            I-GPE-SUBJ\n",
      "▁Cla             I-GPE-SUBJ\n",
      "uß               I-GPE-SUBJ\n",
      "thal             I-GPE-SUBJ\n",
      "▁/               O\n",
      "▁an              O\n",
      "getreten         O\n",
      "en               O\n",
      "▁ander           O\n",
      "n                O\n",
      "▁Eh              O\n",
      "▁/               O\n",
      "▁Mit             O\n",
      "▁der             O\n",
      "▁...             O\n",
      "▁Jung            O\n",
      "fer              O\n",
      "▁Christine       B-PERSON-SUBJ\n",
      "n                B-PERSON-SUBJ\n",
      "▁/               O\n",
      "▁Des             O\n",
      "▁...             O\n",
      "▁Herr            O\n",
      "n                O\n",
      "▁Matthias        B-PERSON-SUBJ\n",
      "▁Toll            I-PERSON-SUBJ\n",
      "ens              I-PERSON-SUBJ\n",
      "▁/               O\n",
      "▁Für             O\n",
      "st               O\n",
      "l                O\n",
      ".                O\n",
      "▁Braun           B-GPE-DES\n",
      "sch              B-GPE-DES\n",
      "w                B-GPE-DES\n",
      ".                B-GPE-DES\n",
      "▁Lü              I-GPE-DES\n",
      "nä               I-GPE-DES\n",
      "burg             I-GPE-DES\n",
      "▁                O\n",
      ".                O\n",
      "▁Richter         O\n",
      "▁und             O\n",
      "▁Hü              O\n",
      "tten             O\n",
      "-                O\n",
      "Re               O\n",
      "uter             O\n",
      "▁da              O\n",
      "sel              O\n",
      "b                O\n",
      "st               O\n",
      "▁/               O\n",
      "▁her             O\n",
      "tz               O\n",
      "ge               O\n",
      "lieb             O\n",
      "ten              O\n",
      "▁Tochter         O\n",
      "▁;               O\n",
      "▁Bey             O\n",
      "▁fe              O\n",
      "yer              O\n",
      "licher           O\n",
      "▁Voll            O\n",
      "zie              O\n",
      "hung             O\n",
      "▁den             O\n",
      "▁2.              B-DATE-SUBJ\n",
      "▁May             I-DATE-SUBJ\n",
      "-                I-DATE-SUBJ\n",
      "Mon              I-DATE-SUBJ\n",
      "ats              I-DATE-SUBJ\n",
      "▁im              O\n",
      "▁16              B-YEAR-SUBJ\n",
      "52               B-YEAR-SUBJ\n",
      "sten             B-YEAR-SUBJ\n",
      "▁Jahre           O\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence, tags = tokenize_and_preserve_labels(df[\"tokens\"][0], df[\"ner_tags\"][0],tokenizer )\n",
    "for token, tag in zip(tokenized_sentence, tags):\n",
    "    print('{0:15}  {1}'.format(token, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6c811c0-2412-49b2-830e-6baace64de4a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'B-DATE-CREATION': 1,\n",
       " 'B-DATE-PUB': 2,\n",
       " 'B-DATE-SUBJ': 3,\n",
       " 'B-GPE-AUT': 4,\n",
       " 'B-GPE-CREATION': 5,\n",
       " 'B-GPE-DES': 6,\n",
       " 'B-GPE-PUB': 7,\n",
       " 'B-GPE-SUBJ': 8,\n",
       " 'B-LITWORK': 9,\n",
       " 'B-ORG-CREATION': 10,\n",
       " 'B-ORG-DES': 11,\n",
       " 'B-ORG-PUB': 12,\n",
       " 'B-ORG-SUBJ': 13,\n",
       " 'B-PERSON-AUT': 14,\n",
       " 'B-PERSON-CON': 15,\n",
       " 'B-PERSON-EDT': 16,\n",
       " 'B-PERSON-PUB': 17,\n",
       " 'B-PERSON-SUBJ': 18,\n",
       " 'B-YEAR-CREATION': 19,\n",
       " 'B-YEAR-PUB': 20,\n",
       " 'B-YEAR-SUBJ': 21,\n",
       " 'I-DATE-CREATION': 22,\n",
       " 'I-DATE-PUB': 23,\n",
       " 'I-DATE-SUBJ': 24,\n",
       " 'I-GPE': 25,\n",
       " 'I-GPE-CREATION': 26,\n",
       " 'I-GPE-DES': 27,\n",
       " 'I-GPE-PUB': 28,\n",
       " 'I-GPE-SUBJ': 29,\n",
       " 'I-I-GPE-DES': 30,\n",
       " 'I-LITWORK': 31,\n",
       " 'I-ORG': 32,\n",
       " 'I-ORG-CREATION': 33,\n",
       " 'I-ORG-DES': 34,\n",
       " 'I-ORG-PUB': 35,\n",
       " 'I-ORG-SUBJ': 36,\n",
       " 'I-PERSON-AUT': 37,\n",
       " 'I-PERSON-CON': 38,\n",
       " 'I-PERSON-EDT': 39,\n",
       " 'I-PERSON-PUB': 40,\n",
       " 'I-PERSON-SUBJ': 41,\n",
       " 'I-YEAR-CREATION': 42,\n",
       " 'I_ORG-DES': 43,\n",
       " 'O': 44}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = { label : ne_labels.index(label) for label in ne_labels}\n",
    "\n",
    "id2label = { ne_labels.index(label) : label for label in ne_labels}\n",
    "\n",
    "label2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f064452d-56c0-4272-ab68-603dd20f0285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pkl_vars(id2label, 'data/vars/id2label')\n",
    "pkl_vars(label2id, 'data/vars/label2id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "df094b9a-e844-44fb-8de7-8acc6499281c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#splite dataset and load for the first time\n",
    "\"\"\"train_dataset = df.sample(frac=0.9,random_state=888)\n",
    "valtest_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "val_dataset = valtest_dataset.sample(frac=0.5,random_state=888)\n",
    "test_dataset = valtest_dataset.drop(val_dataset.index).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "val_dataset = val_dataset.reset_index(drop=True)\n",
    "test_dataset = test_dataset.reset_index(drop=True)\n",
    "\n",
    "#save the data sets\n",
    "train_dataset.to_csv('data/train.csv', index=False)\n",
    "val_dataset.to_csv('data/val.csv', index=False)\n",
    "test_dataset.to_csv('data/test.csv', index=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee1bbcc1-d962-481a-bce0-5040d369e776",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL TrainigDataset: (1000, 4)\n",
      "TRAIN Dataset: (900, 4)\n",
      "VALIDATION Dataset: (50, 4)\n",
      "TEST Dataset: (50, 4)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print(\"FULL TrainigDataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(val_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "12554437-dc58-4740-bc3c-f83259f1a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "validation_set = dataset(val_dataset, tokenizer, MAX_LEN)\n",
    "test_set = dataset(test_dataset, tokenizer, MAX_LEN)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5e35410-9ee9-42e6-ae7c-f8a4a83ef546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tag in ne_labels:\n",
    "    df[f'has_tag_{tag}'] = df['ner_tags'].apply(lambda x: tag in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f406142d-c664-449e-9ab9-fb4f4f8975d9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[~df['has_tag_']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "32d12ec4-90c4-4ca3-bfd4-1eb0ecb64230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#5 fold splite dataset and load for the first time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "skf = KFold(n_splits=5, random_state=888, shuffle=True)\n",
    "n = 1                                                        \n",
    "for trainval_index, test_index in skf.split(X=df['tokens'].to_numpy(), y=df['ner_tags'].to_numpy()):\n",
    "    trainval_set = df.iloc[trainval_index]\n",
    "    train_set = trainval_set[:-100]\n",
    "    val_set = trainval_set[-100:]\n",
    "    test_set = df.iloc[test_index]\n",
    "    train_file_name = f'data/5-fold/train_{n}.csv'\n",
    "    train_set.to_csv(train_file_name, index = False)\n",
    "    val_file_name = f'data/5-fold/val_{n}.csv'\n",
    "    val_set.to_csv(val_file_name, index = False)\n",
    "    test_file_name = f'data/5-fold/test_{n}.csv' \n",
    "    test_set.to_csv(test_file_name, index = False)\n",
    "    n += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b3f4a-7405-4dae-a6f9-147c8354c056",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "## verify tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b97092e0-b064-4a01-9bbd-6faa297dd152",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 527, 13855, 59134, 152201, 4, 76593, 19, 41505, 745, 86829, 13653, 8402, 3157, 53047, 5581, 64181, 1760, 84838, 647, 4097, 112350, 33, 4, 84838, 13206, 76830, 165, 84838, 429, 1566, 42, 510, 27420, 19, 4, 27420, 19, 67538, 14949, 29036, 4, 84838, 919, 7859, 271, 141, 5, 164034, 9, 1456, 117184, 33, 7510, 7, 5, 148713, 101445, 7, 4, 165, 226741, 7, 224, 58836, 11443, 927, 7372, 39215, 164, 28480, 4, 84838, 814, 6651, 1179, 33, 81937, 7, 165, 4265, 73, 132235, 1755, 1225, 921, 43552, 7, 122, 120279, 429, 1566, 42, 16587, 186, 53, 745, 821, 16591, 7655, 8055, 20756, 756, 88394, 4, 1858, 168, 8892, 13328, 5, 14197, 7418, 7, 72, 75900, 67, 4, 729, 11548, 893, 68, 5094, 1991, 9136, 5486, 1600, 42, 23167, 67, 52334, 18, 1177, 165, 9493, 567, 1177, 53, 19059, 27802, 224, 6, 178490, 12512, 62850, 71493, 14, 491, 33312, 872, 72, 2347, 379, 510, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(df[\"sentence\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6a25a884-a02e-4e70-976a-18d9bef07ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_set = dataset(pd.read_csv('data/5-fold/val_1.csv', converters={\"tokens\":literal_eval, \"ner_tags\":literal_eval}), tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4fa70fcf-1182-4c0d-8823-f2d464733820",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>            O\n",
      "▁Von             O\n",
      "▁Gottes          O\n",
      "▁G               O\n",
      "na               O\n",
      "den              O\n",
      "▁                O\n",
      ",                O\n",
      "▁Friedrich       B-PERSON-AUT\n",
      "▁August          I-PERSON-AUT\n",
      "▁                O\n",
      ",                O\n",
      "▁König           O\n",
      "▁in              O\n",
      "▁Po              B-GPE-DES\n",
      "hlen             B-GPE-DES\n",
      "▁                O\n",
      ",                O\n",
      "▁[               O\n",
      "et               O\n",
      "]                O\n",
      "c                O\n",
      ".                O\n",
      "▁Her             O\n",
      "tz               O\n",
      "og               O\n",
      "▁zu              O\n",
      "▁Sachsen         B-GPE-DES\n",
      "▁                O\n",
      ",                O\n",
      "▁J               B-GPE-DES\n",
      "ü                B-GPE-DES\n",
      "lich             B-GPE-DES\n",
      "▁                O\n",
      ",                O\n",
      "▁Cle             B-GPE-DES\n",
      "ve               B-GPE-DES\n",
      "▁                O\n",
      ",                O\n",
      "▁Berg            B-GPE-DES\n",
      "▁                O\n",
      ",                O\n",
      "▁Eng             B-GPE-DES\n",
      "ern              B-GPE-DES\n",
      "▁und             O\n",
      "▁West            B-GPE-DES\n",
      "phal             B-GPE-DES\n",
      "en               B-GPE-DES\n",
      "▁                O\n",
      ",                O\n",
      "▁[               O\n",
      "et               O\n",
      "]                O\n",
      "c                O\n",
      ".                O\n",
      "▁Chu             O\n",
      "r                O\n",
      "-                O\n",
      "F                O\n",
      "ür               O\n",
      "st               O\n",
      "▁                O\n",
      ",                O\n",
      "▁[               O\n",
      "et               O\n",
      "]                O\n",
      "c                O\n",
      ".                O\n",
      "▁Liebe           O\n",
      "▁get             O\n",
      "re               O\n",
      "ue               O\n",
      "▁                O\n",
      ".                O\n",
      "▁Es              O\n",
      "▁ist             O\n",
      "▁ang             O\n",
      "eze              O\n",
      "i                O\n",
      "get              O\n",
      "▁worden          O\n",
      "▁                O\n",
      ",                O\n",
      "▁was             O\n",
      "maa              O\n",
      "ßen              O\n",
      "▁                O\n",
      ",                O\n",
      "▁wenn            O\n",
      "▁einige          O\n",
      "▁Mann            O\n",
      "schaff           O\n",
      "t                O\n",
      "▁von             O\n",
      "▁Unsere          O\n",
      "r                O\n",
      "▁Mili            O\n",
      "z                O\n",
      "▁                O\n",
      ",                O\n",
      "▁the             O\n",
      "ils              O\n",
      "▁wegen           O\n",
      "▁A               O\n",
      "nsä              O\n",
      "ß                O\n",
      "igkeit           O\n",
      "▁                O\n",
      ",                O\n",
      "▁the             O\n",
      "ils              O\n",
      "▁anderer         O\n",
      "▁Ursache         O\n",
      "n                O\n",
      "▁hal             O\n",
      "ber              O\n",
      "▁                O\n",
      ",                O\n",
      "▁um              O\n",
      "b                O\n",
      "▁ihre            O\n",
      "▁Ab              O\n",
      "schied           O\n",
      "e                O\n",
      "▁ange            O\n",
      "suche            O\n",
      "t                O\n",
      "▁                O\n",
      ",                O\n",
      "▁die             O\n",
      "▁...             O\n",
      "▁                O\n",
      ".                O\n",
      "▁er              O\n",
      "the              O\n",
      "ilte             O\n",
      "▁Zeug            O\n",
      "ni               O\n",
      "ß                O\n",
      "e                O\n",
      "▁nicht           O\n",
      "▁durch           O\n",
      "gehend           O\n",
      "s                O\n",
      "▁von             O\n",
      "▁der             O\n",
      "▁Be              O\n",
      "schaffen         O\n",
      "heit             O\n",
      "▁gewesen         O\n",
      "▁                O\n",
      ",                O\n",
      "▁daß             O\n",
      "▁solchen         O\n",
      "▁hin             O\n",
      "läng             O\n",
      "licher           O\n",
      "▁Glau            O\n",
      "be               O\n",
      "▁be              O\n",
      "y                O\n",
      "ge               O\n",
      "messen           O\n",
      "▁...             O\n",
      "▁werden          O\n",
      "▁können          O\n",
      "▁:               O\n",
      "▁[               O\n",
      "▁Ge              O\n",
      "ben              O\n",
      "▁zu              O\n",
      "▁Dre             B-GPE-CREATION\n",
      "ß                B-GPE-CREATION\n",
      "den              B-GPE-CREATION\n",
      "▁                O\n",
      ",                O\n",
      "▁den             O\n",
      "▁4.              B-DATE-CREATION\n",
      "▁Mart            I-DATE-CREATION\n",
      ".                I-DATE-CREATION\n",
      "▁Ann             I-DATE-CREATION\n",
      ".                I-DATE-CREATION\n",
      "▁17              B-YEAR-CREATION\n",
      "44               B-YEAR-CREATION\n",
      "▁                O\n",
      ".                O\n",
      "▁]               O\n",
      "<unk>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n",
      "<pad>            O\n"
     ]
    }
   ],
   "source": [
    "# print the first 50 tokens and corresponding labels\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(validation_set[14][\"ids\"]), validation_set[14][\"targets\"]):\n",
    "    print('{0:15}  {1}'.format(token, id2label[label.item()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7cfd8f-b7d6-4270-8d57-ed6073369319",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dc4847da-e489-42ab-822a-f7f8d51ca8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intial loss = 4.148297309875488\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ids = validation_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = validation_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = validation_set[0][\"targets\"].unsqueeze(0)\n",
    "\n",
    "ids = ids.to(device)#, dtype = torch.long)\n",
    "mask = mask.to(device)#, dtype = torch.long)\n",
    "targets = targets.to(device)#, dtype = torch.long)\n",
    "model.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "\n",
    "print(f\"intial loss = {initial_loss.item()}\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e02b37e9-afcc-46c4-be82-1b36d27784bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8066624897703196"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "-math.log(1/45) # 45 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69497c45-eeb8-42a5-8fa6-947cdc18bb52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(model, training_loader, optimizer, scheduler=None):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "        '''\n",
    "        loss, tr_logits  = model(input_ids=ids, attention_mask=mask, labels=targets)#temporary modification for transformer 3'''\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "        \n",
    "        #if idx % 100==0:\n",
    "        #    loss_step = tr_loss/nb_tr_steps\n",
    "        #    print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "        \n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    #print(f\"Trained {nb_tr_steps} steps\")\n",
    "    logger.info(f\"Training loss epoch: {epoch_loss}\")\n",
    "    logger.info(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    \n",
    "\n",
    "def valid(model, validation_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(validation_loader):\n",
    "            \n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "           \n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "        \n",
    "            #if idx % 100==0:\n",
    "            #    loss_step = eval_loss/nb_eval_steps\n",
    "            #    print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    logger.info(f\"Validation Loss: {eval_loss}\")\n",
    "    logger.info(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions\n",
    "\n",
    "def print_reports_to_csv(test_results, model_name, LEARNING_RATE, EPOCHS, trainset_num, batch_size=TRAIN_BATCH_SIZE, report_type=\"\"):\n",
    "    test_reports = []\n",
    "    for res in test_results:\n",
    "        report = classification_report([res['labels']], [res['predictions']], output_dict=True, zero_division=1)#, scheme=scheme.IOB2, mode='strict')\n",
    "        flattened_report = {str(k+'_'+v_k) : v_v for k,v in report.items() for v_k, v_v in v.items()  }\n",
    "        flattened_report['trainset_size'] = res['trainset_size']\n",
    "        flattened_report['model'] = res['model']\n",
    "        flattened_report['trainset_num'] = trainset_num\n",
    "        test_reports.append(flattened_report)\n",
    "    \n",
    "    df_test_reports = pd.DataFrame(test_reports)\n",
    "    if '/' in model_name:\n",
    "        model_name =  model_name.split('/')[1] \n",
    "    test_report_name = f'finetuning_results/{report_type}_{model_name }_set-{trainset_num}_{LEARNING_RATE}_{batch_size}_{EPOCHS}.csv'\n",
    "    df_test_reports.to_csv(test_report_name, mode='a', header=not os.path.exists(test_report_name),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "263de7a3-9697-4d47-91de-c17d46cd980c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12223, 12223)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set = dataset(pd.read_csv('data/5-fold/val_5.csv', \n",
    "                                     converters={\"tokens\":literal_eval, \n",
    "                                                 \"ner_tags\":literal_eval}),\n",
    "                           tokenizer, MAX_LEN)\n",
    "validation_loader = DataLoader(validation_set, **val_params)\n",
    "val_labels, val_predictions =valid(model, validation_loader)\n",
    "\n",
    "len(val_labels), len(val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "70fe6147-7fd0-4910-bfb5-a0f7c7f4f1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_reports_to_csv([{'trainset_size': trainsetsize, 'model': model_name, 'labels': val_labels, 'predictions': val_predictions}],\n",
    "                     model_name, LEARNING_RATE, EPOCHS, 2, batch_size=TRAIN_BATCH_SIZE, report_type=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e54d2b35-cae2-4661-ba3a-05cda2083199",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_ORG-DES seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_ORG-DES seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_ORG-DES seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56min 59s, sys: 10min 1s, total: 1h 7min 1s\n",
      "Wall time: 1h 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#training\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "            'shuffle': False,\n",
    "            'num_workers': 0\n",
    "            }\n",
    "\n",
    "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "             }\n",
    "\n",
    "\n",
    "for trainset_num in range(2,6):\n",
    "\n",
    "    trainval_file_name = f'data/5-fold/train_{trainset_num}.csv'\n",
    "    val_file_name = f'data/5-fold/val_{trainset_num}.csv'\n",
    "    test_file_name = f'data/5-fold/test_{trainset_num}.csv'\n",
    "    \n",
    "    for model_name in [\"distilbert/distilbert-base-multilingual-cased\",\n",
    "                      \"dbmdz/bert-base-german-cased\",\n",
    "                      \"deepset/bert-base-german-cased-oldvocab\"]:#\"dbmdz/bert-base-german-europeana-cased\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, from_tf=False, model_max_length=MAX_LEN)\n",
    "        df_training_set = pd.read_csv(trainval_file_name,converters={\"tokens\":literal_eval, \"ner_tags\":literal_eval})        \n",
    "        validation_set = dataset(pd.read_csv(val_file_name, converters={\"tokens\":literal_eval, \"ner_tags\":literal_eval}),\n",
    "                           tokenizer, MAX_LEN)\n",
    "        test_set = dataset(pd.read_csv(test_file_name, converters={\"tokens\":literal_eval, \"ner_tags\":literal_eval}),\n",
    "                           tokenizer, MAX_LEN)\n",
    "        \n",
    "        val_results = []\n",
    "        test_results = []\n",
    "        \n",
    "        validation_loader = DataLoader(validation_set, **val_params)\n",
    "        test_loader = DataLoader(test_set, **val_params)\n",
    "        \n",
    "        for trainsetsize in [168, 338, 698]:  # till 168, 338, 698\n",
    "            training_set = dataset(df_training_set[:trainsetsize], tokenizer, MAX_LEN)\n",
    "        \n",
    "            logger.info(\"TRAIN Dataset: {}\".format(training_set.data.shape))\n",
    "            #train_params['batch_size'] =  int( trainsetsize / 32) if (trainsetsize < 1024) else 16\n",
    "            training_loader = DataLoader(training_set, **train_params)\n",
    "        \n",
    "        \n",
    "            num_training_steps = int(training_loader.dataset.len / train_params['batch_size'] * EPOCHS)\n",
    "            logger.info(f'tranining steps: {num_training_steps+1}')\n",
    "        \n",
    "            #Shrey uses TF model\n",
    "            model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "                                                                    from_tf=False,\n",
    "                                                                    num_labels=len(id2label),\n",
    "                                                                    id2label=id2label,\n",
    "                                                                    label2id=label2id)\n",
    "            model.to(device)\n",
    "        \n",
    "            optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "            #scheduler = get_cosine_schedule_with_warmup(optimizer = optimizer, num_warmup_steps = 50, num_training_steps=num_training_steps)\n",
    "            for epoch in range(EPOCHS):\n",
    "            #for epoch in range(flex_epoch_nb): \n",
    "                logger.info(f\"Training epoch: {epoch + 1}\")\n",
    "                train(model, training_loader, optimizer)\n",
    "                valid(model, validation_loader)\n",
    "            val_labels, val_predictions = valid(model, validation_loader)     \n",
    "            val_results.append({'trainset_size': trainsetsize, 'model': model_name, 'labels': val_labels, 'predictions': val_predictions})\n",
    "        \n",
    "            #test generalizablity\n",
    "            test_labels, test_predictions = valid(model, test_loader)\n",
    "            test_results.append({'trainset_size': trainsetsize, 'model': model_name, 'labels': test_labels, 'predictions': test_predictions})\n",
    "            \n",
    "            ner_model_name = f'ft_models/{model_name.split(\"/\")[-1]}_ft_{EPOCHS}ep_train_size_{trainsetsize}_trainset_{trainset_num}'\n",
    "            model.save_pretrained(ner_model_name)\n",
    "            tokenizer.save_pretrained(ner_model_name)\n",
    "            \n",
    "            # gpt_aligned_eval(model, tokenizer, ner_model_name) # too slow!\n",
    "        \n",
    "        print_reports_to_csv(val_results, model_name, LEARNING_RATE, EPOCHS, trainset_num, report_type='validation')\n",
    "        print_reports_to_csv(test_results, model_name, LEARNING_RATE, EPOCHS, trainset_num, report_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c756958c-788f-4384-a87a-a7ed4e865e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255fd88-52a6-4113-a2dd-30fea32a843e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
